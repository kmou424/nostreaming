# nostreaming

ä¸€ä¸ªé€šç”¨çš„ LLM API åå‘ä»£ç†å·¥å…·ï¼Œæ”¯æŒå¤šä¸ªä¸Šæ¸¸æä¾›å•†ï¼Œæä¾› OpenAI å…¼å®¹çš„æ¥å£ï¼Œå¹¶å®ç°ä¼ªæµå¼ï¼ŒæŠŠä½ çš„åªæ”¯æŒæµå¼å“åº”çš„ API å…¨éƒ¨è½¬æ¢ä¸ºä¼ªæµå¼å“åº”ã€‚ä¸»è¦ä¸º Gemini è®¾è®¡ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ”„ **å¤šæä¾›å•†æ”¯æŒ**: æ”¯æŒ OpenAI ç­‰(å’•å’•å’•)ä¸Šæ¸¸ LLM æä¾›å•†
- ğŸ”Œ **OpenAI å…¼å®¹æ¥å£**: æä¾›æ ‡å‡†çš„ OpenAI API æ¥å£ï¼Œæ— éœ€ä¿®æ”¹å®¢æˆ·ç«¯ä»£ç 
- ğŸ“¡ **Fake-Streaming**: å®ç°ä¼ªæµå¼ä¼ è¾“ï¼Œå®¢æˆ·ç«¯é€šè¿‡ SSE è¿æ¥ï¼Œåç«¯å‘é€éæµå¼è¯·æ±‚åˆ°ä¸Šæ¸¸
- ğŸ”„ **è‡ªåŠ¨é‡è¯•**: ç©ºå“åº”æ£€æµ‹å¹¶è‡ªåŠ¨é‡è¯•è¯·æ±‚
- ğŸ•µï¸â€â™‚ï¸ **è¯·æ±‚å†…å®¹ä¼ªé€ **: ä¼ªé€ è¯·æ±‚å†…å®¹ï¼Œä»¥é¿å…æ£€æµ‹ï¼ˆThanks to [hajimi](https://github.com/wyeeeee/hajimi/blob/3712ba496c9d16a62f4017fbeb41d6d3bd36bb5b/app/services/gemini.py#L516)ï¼‰
- ğŸ” **API å¯†é’¥è®¤è¯**: æ”¯æŒ Bearer token è®¤è¯
- ğŸ¯ **æ¨¡å‹è¿‡æ»¤**: æ”¯æŒç™½åå•/é»‘åå•æ¨¡å¼è¿‡æ»¤æ¨¡å‹
- ğŸ³ **Docker æ”¯æŒ**: æä¾›å®Œæ•´çš„ Docker éƒ¨ç½²æ–¹æ¡ˆ
- ğŸ“Š **ç»“æ„åŒ–æ—¥å¿—**: åŸºäºé…ç½®çš„æ—¥å¿—çº§åˆ«å’Œç»“æ„åŒ–è¾“å‡º

## å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

- [Bun](https://bun.sh) >= 1.0.0

### æœ¬åœ°è¿è¡Œ

1. **å®‰è£…ä¾èµ–**

```bash
bun install
```

2. **é…ç½®åº”ç”¨**

```bash
cp config.toml.example config.toml
# ç¼–è¾‘ config.toml é…ç½®ä½ çš„æä¾›å•†ä¿¡æ¯
```

3. **å¯åŠ¨æœåŠ¡**

```bash
bun run src/index.ts
```

æœåŠ¡å°†åœ¨ `http://localhost:3000` å¯åŠ¨ã€‚

## é…ç½®è¯´æ˜

é…ç½®æ–‡ä»¶ä½¿ç”¨ TOML æ ¼å¼ï¼Œä¸»è¦é…ç½®é¡¹ï¼š

```toml
[app]
host = "0.0.0.0"
port = 3000
keys = ["sk-XXXXX"]  # API å¯†é’¥åˆ—è¡¨ï¼Œç”¨äºè®¤è¯
fakeStreamInterval = 500  # fake-streaming ä¿æŒè¿æ¥é—´éš”ï¼ˆæ¯«ç§’ï¼‰

[logging]
level = "info"  # debug, info, warn, error

[providers.my-openai]
enabled = true
type = "openai"
endpoint = "https://api.openai.com/v1"
api_key = "sk-proj-1234567890"

[providers.my-openai.filter]
mode = "whitelist"  # whitelist æˆ– blacklist
models = ["gpt-4o", "gpt-4o-mini"]
```

è¯¦ç»†é…ç½®è¯´æ˜è¯·å‚è€ƒ `config.toml.example`ã€‚

## éƒ¨ç½²æ–¹å¼

### Docker éƒ¨ç½²ï¼ˆæ¨èï¼‰

#### ä½¿ç”¨ Docker Compose

```bash
# å‡†å¤‡é…ç½®æ–‡ä»¶
cp config.toml.example config.toml
# ç¼–è¾‘ config.toml

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# åœæ­¢æœåŠ¡
docker-compose down
```

#### ä½¿ç”¨ Docker

```bash
# æ„å»ºé•œåƒ
docker build -t nostreaming:latest .

# è¿è¡Œå®¹å™¨
docker run -d \
  --name nostreaming \
  -p 3000:3000 \
  -v $(pwd)/config.toml:/app/config.toml:ro \
  nostreaming:latest
```

è¯¦ç»†çš„ Docker éƒ¨ç½²æŒ‡å—è¯·å‚è€ƒ [README.docker.md](./README.docker.md)ã€‚

### æœ¬åœ°éƒ¨ç½²

```bash
# å®‰è£…ä¾èµ–
bun install

# é…ç½®åº”ç”¨
cp config.toml.example config.toml
# ç¼–è¾‘ config.toml

# å¯åŠ¨æœåŠ¡
bun run src/index.ts
```

## æ›´æ–°åº”ç”¨

### 1. æ‹‰å–æœ€æ–°ä»£ç 

```bash
# æ‹‰å–æœ€æ–°ä»£ç 
git pull
```

### 2. æŒ‰éƒ¨ç½²æ–¹å¼æ›´æ–°

#### æœ¬åœ°éƒ¨ç½²

```bash
# 1. å®‰è£…/æ›´æ–°ä¾èµ–ï¼ˆå¦‚æœæœ‰å˜æ›´ï¼‰
bun install

# 2. é‡å¯æœåŠ¡
# å¦‚æœä½¿ç”¨è¿›ç¨‹ç®¡ç†å™¨ï¼ˆå¦‚ PM2ï¼‰ï¼Œé‡å¯è¿›ç¨‹
# å¦‚æœç›´æ¥è¿è¡Œï¼Œåœæ­¢æ—§è¿›ç¨‹åé‡æ–°å¯åŠ¨
bun run src/index.ts
```

#### Docker éƒ¨ç½² - ä½¿ç”¨ Docker Compose

```bash
# 1. åœæ­¢å¹¶åˆ é™¤æ—§å®¹å™¨
docker-compose down

# 2. åˆ é™¤æ—§é•œåƒï¼ˆä»¥æœ¬åœ°æ„å»ºçš„é•œåƒåä¸ºå‡†ï¼‰
docker rmi nostreaming:latest

# 3. é‡æ–°æ„å»ºé•œåƒ
docker-compose build

# 4. å¯åŠ¨æ–°å®¹å™¨
docker-compose up -d

# 5. æŸ¥çœ‹æ—¥å¿—ç¡®è®¤æ›´æ–°æˆåŠŸ
docker-compose logs -f
```

#### Docker éƒ¨ç½² - ä½¿ç”¨ Docker

```bash
# 1. åœæ­¢å¹¶åˆ é™¤æ—§å®¹å™¨
docker stop nostreaming
docker rm nostreaming

# 2. åˆ é™¤æ—§é•œåƒï¼ˆä»¥æœ¬åœ°æ„å»ºçš„é•œåƒåä¸ºå‡†ï¼‰
docker rmi nostreaming:latest

# 3. é‡æ–°æ„å»ºé•œåƒ
docker build -t nostreaming:latest .

# 4. å¯åŠ¨æ–°å®¹å™¨
docker run -d \
  --name nostreaming \
  -p 3000:3000 \
  -v $(pwd)/config.toml:/app/config.toml:ro \
  nostreaming:latest

# 5. æŸ¥çœ‹æ—¥å¿—ç¡®è®¤æ›´æ–°æˆåŠŸ
docker logs -f nostreaming
```

**æ³¨æ„**ï¼š

- æ›´æ–°å‰å»ºè®®å¤‡ä»½ `config.toml` é…ç½®æ–‡ä»¶
- å¦‚æœé…ç½®æ–‡ä»¶æ ¼å¼æœ‰å˜æ›´ï¼Œè¯·å‚è€ƒ `config.toml.example` æ›´æ–°é…ç½®
- æ›´æ–°åå¯é€šè¿‡æ—¥å¿—ä¸­çš„ç‰ˆæœ¬å·ç¡®è®¤æ˜¯å¦æ›´æ–°æˆåŠŸ

## API ä½¿ç”¨

### å¥åº·æ£€æŸ¥

```bash
curl http://localhost:3000/health
```

### è·å–æ¨¡å‹åˆ—è¡¨

```bash
curl -H "Authorization: Bearer sk-XXXXX" \
  http://localhost:3000/v1/models
```

### èŠå¤©è¡¥å…¨

```bash
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Authorization: Bearer sk-XXXXX" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "my-openai/gpt-4o",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

### æµå¼è¯·æ±‚

```bash
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Authorization: Bearer sk-XXXXX" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "my-openai/gpt-4o",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "stream": true
  }'
```

## æ¨¡å‹å‘½åè§„åˆ™

æ¨¡å‹åç§°æ ¼å¼ï¼š`{provider_name}/{model_id}`

ä¾‹å¦‚ï¼š

- `my-openai/gpt-4o` - ä½¿ç”¨ `my-openai` æä¾›å•†çš„ `gpt-4o` æ¨¡å‹
- `my-openai/gpt-4o-mini` - ä½¿ç”¨ `my-openai` æä¾›å•†çš„ `gpt-4o-mini` æ¨¡å‹

## æŠ€æœ¯æ ˆ

- **Runtime**: Bun
- **HTTP Framework**: Elysia
- **é…ç½®**: TOML
- **HTTP Client**: Axios
- **éªŒè¯**: Zod

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE)ã€‚
